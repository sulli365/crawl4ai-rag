"""
GitHub Documentation scraper for {{ url }}
Purpose: {{ purpose }}
Generated by crawl4ai-rag
"""

import asyncio
import json
import os
import re
import base64
from typing import Dict, List, Any, Optional, Tuple
from urllib.parse import urljoin, urlparse

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from loguru import logger

# Import Supabase integration
from supabase import create_client, Client

# Import MCP client utilities if available
try:
    from analyzer.github_mcp_service import GitHubMcpService
    HAS_MCP = True
except ImportError:
    HAS_MCP = False
    logger.warning("MCP client utilities not available, falling back to direct crawling")

class GitHubDocumentationScraper:
    """
    Specialized scraper for GitHub documentation at {{ url }}.
    Purpose: {{ purpose }}
    """
    
    def __init__(
        self, 
        output_dir: str = "./output",
        supabase_url: Optional[str] = None,
        supabase_key: Optional[str] = None,
        github_token: Optional[str] = None,
        use_mcp: bool = True
    ):
        """
        Initialize the GitHub documentation scraper.
        
        Args:
            output_dir: Directory to save extracted documentation
            supabase_url: Supabase URL (if None, will use environment variable)
            supabase_key: Supabase key (if None, will use environment variable)
            github_token: GitHub token (if None, will use environment variable)
            use_mcp: Whether to use MCP for GitHub API access
        """
        self.output_dir = output_dir
        self.base_url = "{{ url }}"
        self.repo_info = self._parse_github_url(self.base_url)
        self.doc_metrics = {}
        self.use_mcp = use_mcp and HAS_MCP
        
        # Initialize GitHub MCP service if available
        self.github_service = None
        if self.use_mcp:
            self.github_service = GitHubMcpService()
        
        # Set up GitHub token
        self.github_token = github_token or os.environ.get("GITHUB_TOKEN")
        if not self.github_token and self.use_mcp:
            logger.warning("GitHub token not provided, some MCP operations may fail")
        
        # Set up Supabase client
        self.supabase_url = supabase_url or os.environ.get("SUPABASE_URL")
        self.supabase_key = supabase_key or os.environ.get("SUPABASE_KEY")
        self.supabase_client = None
        
        if self.supabase_url and self.supabase_key:
            try:
                self.supabase_client = create_client(self.supabase_url, self.supabase_key)
                logger.info("Supabase client initialized successfully")
            except Exception as e:
                logger.error(f"Failed to initialize Supabase client: {str(e)}")
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Create subdirectories for different content types
        self.api_dir = os.path.join(output_dir, "api")
        self.code_dir = os.path.join(output_dir, "code_examples")
        self.readme_dir = os.path.join(output_dir, "readme")
        
        os.makedirs(self.api_dir, exist_ok=True)
        os.makedirs(self.code_dir, exist_ok=True)
        os.makedirs(self.readme_dir, exist_ok=True)
    
    def _parse_github_url(self, url: str) -> Dict[str, str]:
        """
        Parse a GitHub URL to extract repository information.
        
        Args:
            url: GitHub URL
            
        Returns:
            Dictionary with repository information
        """
        parsed = urlparse(url)
        path_parts = parsed.path.strip("/").split("/")
        
        if len(path_parts) < 2:
            raise ValueError(f"Invalid GitHub URL: {url}")
        
        owner = path_parts[0]
        repo = path_parts[1]
        
        # Determine if this is a specific path within the repo
        path = "/".join(path_parts[2:]) if len(path_parts) > 2 else ""
        
        # Determine if this is a specific branch
        branch = "{{ repo_info.branch }}"  # Use branch from repo_info
        if len(path_parts) > 4 and path_parts[2] == "tree":
            branch = path_parts[3]
            path = "/".join(path_parts[4:])
        
        return {
            "owner": owner,
            "repo": repo,
            "branch": branch,
            "path": path,
            "full_repo": f"{owner}/{repo}"
        }
    
    async def scrape_documentation(self) -> Dict[str, Any]:
        """
        Scrape the GitHub documentation and extract structured content.
        
        Returns:
            Dictionary with extracted documentation data
        """
        try:
            # Use MCP if available and enabled
            if self.use_mcp:
                logger.info(f"Using MCP to scrape GitHub repository: {self.repo_info['full_repo']}")
                return await self._scrape_with_mcp()
            else:
                logger.info(f"Using direct crawling for GitHub repository: {self.repo_info['full_repo']}")
                return await self._scrape_with_crawler()
                
        except Exception as e:
            logger.error(f"Error scraping documentation: {str(e)}")
            return {"error": str(e)}
    
    async def _scrape_with_mcp(self) -> Dict[str, Any]:
        """
        Scrape GitHub documentation using MCP.
        
        Returns:
            Dictionary with extracted documentation data
        """
        try:
            # Get repository information
            repo_result = await self.github_service.search_repositories(
                query=f"repo:{self.repo_info['owner']}/{self.repo_info['repo']}"
            )
            
            if "error" in repo_result:
                logger.error(f"Error getting repository info: {repo_result['error']}")
                # Fall back to direct crawling
                logger.info("Falling back to direct crawling")
                return await self._scrape_with_crawler()
            
            if not repo_result.get("items"):
                logger.error(f"Repository {self.repo_info['full_repo']} not found")
                return {"error": f"Repository {self.repo_info['full_repo']} not found"}
            
            repo_data = repo_result["items"][0]
            
            # Get README content
            readme_result = await self.github_service.get_file_contents(
                owner=self.repo_info["owner"],
                repo=self.repo_info["repo"],
                path="README.md",
                branch=self.repo_info["branch"]
            )
            
            readme_content = ""
            if not "error" in readme_result and readme_result.get("content"):
                # Decode content if it's base64 encoded
                content = readme_result.get("content", "")
                if readme_result.get("encoding") == "base64":
                    readme_content = base64.b64decode(content).decode("utf-8")
                else:
                    readme_content = content
            
            # Process README content
            doc_structure = self._analyze_documentation(readme_content)
            api_sections = self._extract_api_sections(readme_content)
            code_examples = self._extract_code_examples(readme_content)
            github_elements = self._extract_github_elements(readme_content)
            
            # Get repository structure (list of files)
            files = []
            try:
                # This is a simplified approach - in a real implementation,
                # you would recursively get all files
                root_contents = await self.github_service.get_file_contents(
                    owner=self.repo_info["owner"],
                    repo=self.repo_info["repo"],
                    path="",
                    branch=self.repo_info["branch"]
                )
                
                if not "error" in root_contents and isinstance(root_contents, list):
                    files = [item.get("path") for item in root_contents if item.get("type") == "file"]
                    
                    # Process documentation files
                    await self._process_documentation_files(files)
            except Exception as e:
                logger.warning(f"Error getting repository structure: {str(e)}")
            
            # Save documentation metrics
            self.doc_metrics = {
                "url": self.base_url,
                "title": f"README - {self.repo_info['full_repo']}",
                "repo": self.repo_info["full_repo"],
                "branch": self.repo_info["branch"],
                "path": self.repo_info["path"],
                "sections": doc_structure["sections"],
                "code_examples": {
                    "count": len(code_examples),
                    "languages": self._count_languages(code_examples)
                },
                "api_sections": len(api_sections),
                "github_elements": github_elements,
                "files": files
            }
            
            # Save README to files
            with open(os.path.join(self.readme_dir, "README.md"), "w", encoding="utf-8") as f:
                f.write(readme_content)
            
            # Save documentation metrics
            with open(os.path.join(self.output_dir, "doc_metrics.json"), "w", encoding="utf-8") as f:
                json.dump(self.doc_metrics, f, indent=2)
            
            # Save to Supabase if client is available
            if self.supabase_client:
                await self._save_to_supabase_mcp(readme_content, doc_structure, api_sections, code_examples)
            
            return {
                "title": f"README - {self.repo_info['full_repo']}",
                "url": self.base_url,
                "repo": self.repo_info["full_repo"],
                "structure": doc_structure,
                "api_sections": api_sections,
                "code_examples": code_examples,
                "github_elements": github_elements,
                "files": files,
                "output_dir": self.output_dir
            }
            
        except Exception as e:
            logger.error(f"Error scraping with MCP: {str(e)}")
            # Fall back to direct crawling
            logger.info("Falling back to direct crawling")
            return await self._scrape_with_crawler()
    
    async def _process_documentation_files(self, files: List[str]) -> None:
        """
        Process documentation files from the repository.
        
        Args:
            files: List of file paths
        """
        # Documentation file extensions
        doc_extensions = {"md", "rst", "txt"}
        
        # Documentation directories
        doc_dirs = {"docs", "doc", "documentation", "wiki"}
        
        for file_path in files:
            # Skip non-documentation files
            ext = os.path.splitext(file_path)[1].lstrip(".").lower()
            path_parts = file_path.lower().split("/")
            
            is_doc_file = (
                ext in doc_extensions or
                any(dir_name in path_parts for dir_name in doc_dirs)
            )
            
            if not is_doc_file:
                continue
            
            # Get file content
            try:
                file_result = await self.github_service.get_file_contents(
                    owner=self.repo_info["owner"],
                    repo=self.repo_info["repo"],
                    path=file_path,
                    branch=self.repo_info["branch"]
                )
                
                if "error" in file_result:
                    logger.warning(f"Error getting file {file_path}: {file_result['error']}")
                    continue
                
                # Decode content if it's base64 encoded
                content = file_result.get("content", "")
                if file_result.get("encoding") == "base64":
                    content = base64.b64decode(content).decode("utf-8")
                
                # Save file to appropriate directory
                output_path = os.path.join(self.output_dir, file_path)
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                
                with open(output_path, "w", encoding="utf-8") as f:
                    f.write(content)
                
                logger.info(f"Saved documentation file: {file_path}")
                
                # Save to Supabase if client is available
                if self.supabase_client:
                    # Prepare metadata
                    metadata = {
                        "repo": self.repo_info["full_repo"],
                        "owner": self.repo_info["owner"],
                        "repo_name": self.repo_info["repo"],
                        "branch": self.repo_info["branch"],
                        "path": file_path,
                        "type": "file",
                        "file_extension": ext,
                        "crawled_at": self._get_current_timestamp(),
                        "content_length": len(content)
                    }
                    
                    # Generate embedding for the content
                    embedding = await self._generate_embedding(content)
                    
                    # Prepare data for insertion
                    url = f"{self.base_url}/blob/{self.repo_info['branch']}/{file_path}"
                    title = f"{os.path.basename(file_path)} - {self.repo_info['full_repo']}"
                    
                    data = {
                        "url": url,
                        "title": title,
                        "content": content,
                        "metadata": metadata,
                        "embedding": embedding
                    }
                    
                    # Insert or update the page
                    self.supabase_client.table("crawl4ai_site_pages").upsert(data).execute()
                    
                    logger.info(f"Saved file to Supabase: {file_path}")
                
            except Exception as e:
                logger.error(f"Error processing file {file_path}: {str(e)}")
    
    async def _scrape_with_crawler(self) -> Dict[str, Any]:
        """
        Scrape GitHub documentation using direct crawling.
        
        Returns:
            Dictionary with extracted documentation data
        """
        # Configure browser
        browser_config = BrowserConfig(
            headless=True,
            verbose=False,
            extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
        )
        
        # Configure crawler with GitHub-specific settings
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            excluded_tags=['form', 'header', 'footer', 'nav'],
            exclude_external_links=True,
            process_iframes=True,
            remove_overlay_elements=True,
        )
        
        try:
            # Create the crawler instance
            async with AsyncWebCrawler(config=browser_config) as crawler:
                # Crawl the URL
                result = await crawler.arun(
                    url=self.base_url,
                    config=crawl_config
                )
                
                if not result.success:
                    logger.error(f"Failed to crawl: {result.error_message}")
                    return {"error": result.error_message}
                
                # Extract documentation structure
                doc_structure = self._analyze_documentation(result.markdown.raw_markdown)
                
                # Extract API sections
                api_sections = self._extract_api_sections(result.markdown.raw_markdown)
                
                # Extract code examples
                code_examples = self._extract_code_examples(result.markdown.raw_markdown)
                
                # Extract GitHub-specific elements
                github_elements = self._extract_github_elements(result.markdown.raw_markdown)
                
                # Save documentation metrics
                self.doc_metrics = {
                    "url": self.base_url,
                    "title": result.title,
                    "repo": self.repo_info["full_repo"],
                    "branch": self.repo_info["branch"],
                    "path": self.repo_info["path"],
                    "sections": doc_structure["sections"],
                    "code_examples": {
                        "count": len(code_examples),
                        "languages": self._count_languages(code_examples)
                    },
                    "api_sections": len(api_sections),
                    "github_elements": github_elements,
                    "internal_links": len(result.links["internal"]),
                    "external_links": len(result.links["external"])
                }
                
                # Save documentation to files
                await self._save_documentation(result, doc_structure, api_sections, code_examples)
                
                # Save to Supabase if client is available
                if self.supabase_client:
                    await self._save_to_supabase(result, doc_structure, api_sections, code_examples)
                
                return {
                    "title": result.title,
                    "url": self.base_url,
                    "repo": self.repo_info["full_repo"],
                    "structure": doc_structure,
                    "api_sections": api_sections,
                    "code_examples": code_examples,
                    "github_elements": github_elements,
                    "output_dir": self.output_dir
                }
                
        except Exception as e:
            logger.error(f"Error scraping with crawler: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_documentation(self, markdown: str) -> Dict[str, Any]:
        """
        Analyze documentation structure from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            Dictionary with documentation structure analysis
        """
        lines = markdown.split("\n")
        sections = []
        current_section = None
        
        for i, line in enumerate(lines):
            if line.startswith("#"):
                # Count heading level
                level = 0
                for char in line:
                    if char == "#":
                        level += 1
                    else:
                        break
                
                # Extract heading text
                heading = line[level:].strip()
                
                # Close previous section if exists
                if current_section:
                    current_section["end_line"] = i - 1
                    current_section["content"] = "\n".join(lines[current_section["start_line"]:i])
                    sections.append(current_section)
                
                # Start new section
                current_section = {
                    "title": heading,
                    "level": level,
                    "start_line": i,
                    "end_line": None,
                    "content": None
                }
        
        # Close the last section
        if current_section:
            current_section["end_line"] = len(lines) - 1
            current_section["content"] = "\n".join(lines[current_section["start_line"]:])
            sections.append(current_section)
        
        return {
            "sections": sections,
            "section_count": len(sections),
            "has_introduction": any(s["title"].lower() in ["introduction", "overview", "about"] for s in sections),
            "has_installation": any(s["title"].lower() in ["installation", "setup", "getting started"] for s in sections),
            "has_api_reference": any(s["title"].lower() in ["api", "reference", "api reference"] for s in sections)
        }
    
    def _extract_api_sections(self, markdown: str) -> List[Dict[str, Any]]:
        """
        Extract API documentation sections from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            List of API sections with metadata
        """
        import re
        
        api_sections = []
        lines = markdown.split("\n")
        
        # Patterns that indicate API documentation
        api_patterns = [
            r"^\s*#{1,3}\s+.*\bAPI\b",
            r"^\s*#{1,3}\s+.*\bEndpoint\b",
            r"^\s*#{1,3}\s+.*\bMethod\b",
            r"^\s*#{1,3}\s+.*\bFunction\b",
            r"^\s*#{1,3}\s+.*\bClass\b",
            r"^\s*`[^`]+`\s*\(",  # Function signature like `function_name(`
        ]
        
        # GitHub-specific API patterns
        github_api_patterns = [
            r"^\s*#{1,3}\s+.*\bParameters\b",
            r"^\s*#{1,3}\s+.*\bReturns\b",
            r"^\s*#{1,3}\s+.*\bExamples\b",
            r"^\s*#{1,3}\s+.*\bUsage\b",
            r"^\s*#{1,3}\s+.*\bOptions\b",
        ]
        
        # Combine all patterns
        all_patterns = api_patterns + github_api_patterns
        
        # Compile patterns for efficiency
        compiled_patterns = [re.compile(pattern) for pattern in all_patterns]
        
        # Track current section
        current_section = None
        
        for i, line in enumerate(lines):
            # Check if line matches any API pattern
            for pattern in compiled_patterns:
                if pattern.search(line):
                    # Extract section name
                    section_name = line.strip("# \t")
                    
                    # If we're already in a section, close it
                    if current_section:
                        current_section["end_line"] = i - 1
                        current_section["content"] = "\n".join(lines[current_section["start_line"]:i])
                        api_sections.append(current_section)
                    
                    # Start new section
                    current_section = {
                        "name": section_name,
                        "start_line": i,
                        "end_line": None,
                        "content": None
                    }
                    break
        
        # Close the last section if there is one
        if current_section:
            current_section["end_line"] = len(lines) - 1
            current_section["content"] = "\n".join(lines[current_section["start_line"]:])
            api_sections.append(current_section)
        
        return api_sections
    
    def _extract_code_examples(self, markdown: str) -> List[Dict[str, Any]]:
        """
        Extract code examples from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            List of code examples with metadata
        """
        import re
        
        code_examples = []
        
        # Extract code blocks with their language
        code_block_pattern = r"```(\w*)\n(.*?)```"
        matches = re.findall(code_block_pattern, markdown, re.DOTALL)
        
        for i, (lang, content) in enumerate(matches):
            lang = lang.lower().strip() or "text"  # Default to "text" if no language specified
            
            # Determine purpose (heuristic)
            purpose = "other"
            content_lower = content.lower()
            if "install" in content_lower or "pip" in content_lower or "npm" in content_lower:
                purpose = "installation"
            elif "example" in content_lower or "# example" in content_lower:
                purpose = "example"
            elif "import" in content_lower and len(content.split("\n")) > 5:
                purpose = "usage"
            elif "def " in content_lower or "class " in content_lower or "function" in content_lower:
                purpose = "api"
            
            # GitHub-specific purpose detection
            if "git clone" in content_lower:
                purpose = "git_clone"
            elif "github.com" in content_lower:
                purpose = "github_reference"
            
            code_examples.append({
                "id": i + 1,
                "language": lang,
                "content": content,
                "purpose": purpose,
                "line_count": len(content.split("\n"))
            })
        
        return code_examples
    
    def _extract_github_elements(self, markdown: str) -> Dict[str, Any]:
        """
        Extract GitHub-specific elements from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            Dictionary with GitHub-specific elements
        """
        import re
        
        # Extract GitHub badges
        badge_pattern = r"!\[([^\]]*)\]\((https?://(?:img\.shields\.io|github\.com)[^\)]+)\)"
        badges = re.findall(badge_pattern, markdown)
        
        # Extract GitHub Actions references
        actions_pattern = r"github\.com/[^/]+/[^/]+/actions|github/workflows|\.github/workflows"
        has_actions = bool(re.search(actions_pattern, markdown))
        
        # Extract GitHub Issues references
        issues_pattern = r"github\.com/[^/]+/[^/]+/issues|#\d+"
        has_issues = bool(re.search(issues_pattern, markdown))
        
        # Extract GitHub Pull Requests references
        pr_pattern = r"github\.com/[^/]+/[^/]+/pull|PR"
        has_prs = bool(re.search(pr_pattern, markdown))
        
        # Extract GitHub Wiki references
        wiki_pattern = r"github\.com/[^/]+/[^/]+/wiki"
        has_wiki = bool(re.search(wiki_pattern, markdown))
        
        # Extract GitHub Releases references
        releases_pattern = r"github\.com/[^/]+/[^/]+/releases|v\d+\.\d+\.\d+"
        has_releases = bool(re.search(releases_pattern, markdown))
        
        # Extract GitHub Discussions references
        discussions_pattern = r"github\.com/[^/]+/[^/]+/discussions"
        has_discussions = bool(re.search(discussions_pattern, markdown))
        
        # Extract GitHub Sponsors references
        sponsors_pattern = r"github\.com/sponsors"
        has_sponsors = bool(re.search(sponsors_pattern, markdown))
        
        # Extract GitHub Pages references
        pages_pattern = r"github\.io"
        has_pages = bool(re.search(pages_pattern, markdown))
        
        # Extract GitHub Projects references
        projects_pattern = r"github\.com/[^/]+/[^/]+/projects"
        has_projects = bool(re.search(projects_pattern, markdown))
        
        # Extract GitHub Security references
        security_pattern = r"github\.com/[^/]+/[^/]+/security|SECURITY\.md"
        has_security = bool(re.search(security_pattern, markdown))
        
        # Extract GitHub Contributing references
        contributing_pattern = r"CONTRIBUTING\.md"
        has_contributing = bool(re.search(contributing_pattern, markdown))
        
        # Extract GitHub Code of Conduct references
        coc_pattern = r"CODE_OF_CONDUCT\.md"
        has_coc = bool(re.search(coc_pattern, markdown))
        
        # Extract GitHub License references
        license_pattern = r"LICENSE|license\."
        has_license = bool(re.search(license_pattern, markdown))
        
        return {
            "badges": badges,
            "has_actions": has_actions,
            "has_issues": has_issues,
            "has_prs": has_prs,
            "has_wiki": has_wiki,
            "has_releases": has_releases,
            "has_discussions": has_discussions,
            "has_sponsors": has_sponsors,
            "has_pages": has_pages,
            "has_projects": has_projects,
            "has_security": has_security,
            "has_contributing": has_contributing,
            "has_coc": has_coc,
            "has_license": has_license
        }
    
    def _count_languages(self, code_examples: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        Count code examples by language.
        
        Args:
            code_examples: List of code examples
            
        Returns:
            Dictionary mapping languages to counts
        """
        language_counts = {}
        for example in code_examples:
            lang = example["language"]
            language_counts[lang] = language_counts.get(lang, 0) + 1
        
        return language_counts
    
    async def _save_documentation(
        self, 
        result: Any, 
        doc_structure: Dict[str, Any],
        api_sections: List[Dict[str, Any]],
        code_examples: List[Dict[str, Any]]
    ) -> None:
        """
        Save extracted documentation to files.
        
        Args:
            result: Crawler result
            doc_structure: Documentation structure analysis
            api_sections: Extracted API sections
            code_examples: Extracted code examples
        """
        # Save full markdown content
        with open(os.path.join(self.output_dir, "full_documentation.md"), "w", encoding="utf-8") as f:
            f.write(result.markdown.raw_markdown)
        
        # Save documentation metrics
        with open(os.path.join(self.output_dir, "doc_metrics.json"), "w", encoding="utf-8") as f:
            json.dump(self.doc_metrics, f, indent=2)
        
        # Save API sections to separate files
        for i, section in enumerate(api_sections):
            filename = f"api_{i+1}_{self._sanitize_filename(section['name'])}.md"
            with open(os.path.join(self.api_dir, filename), "w", encoding="utf-8") as f:
                f.write(section["content"])
        
        # Save code examples to separate files
        for example in code_examples:
            ext = example["language"] if example["language"] != "text" else "txt"
            filename = f"example_{example['id']}_{example['purpose']}.{ext}"
            with open(os.path.join(self.code_dir, filename), "w", encoding="utf-8") as f:
                f.write(example["content"])
        
        # Save README if this is a repository root
        if not self.repo_info["path"] and doc_structure["sections"]:
            with open(os.path.join(self.readme_dir, "README.md"), "w", encoding="utf-8") as f:
                f.write(result.markdown.raw_markdown)
    
    async def _save_to_supabase_mcp(
        self,
        content: str,
        doc_structure: Dict[str, Any],
        api_sections: List[Dict[str, Any]],
        code_examples: List[Dict[str, Any]]
    ) -> None:
        """
        Save documentation to Supabase using MCP data.
        
        Args:
            content: Markdown content
            doc_structure: Documentation structure analysis
            api_sections: Extracted API sections
            code_examples: Extracted code examples
        """
        try:
            # Generate embedding for the content
            embedding = await self._generate_embedding(content)
            
            # Prepare metadata
            metadata = {
                "repo": self.repo_info["full_repo"],
                "owner": self.repo_info["owner"],
                "repo_name": self.repo_info["repo"],
                "branch": self.repo_info["branch"],
                "path": self.repo_info["path"],
                "crawled_at": self._get_current_timestamp(),
                "content_length": len(content),
                "doc_metrics": self.doc_metrics,
                "api_sections_count": len(api_sections),
                "code_examples_count": len(code_examples),
                "source": "mcp"
            }
            
            # Prepare data for insertion
            data = {
                "url": self.base_url,
                "title": f"README - {self.repo_info['full_repo']}",
                "content": content,
                "metadata": metadata,
                "embedding": embedding
            }
            
            # Insert or update the page
            self.supabase_client.table("crawl4ai_site_pages").upsert(data).execute()
            
            logger.info(f"Saved documentation to Supabase: {self.base_url}")
            
            # Save API sections as separate entries if they're substantial
            for i, section in enumerate(api_sections):
                if len(section["content"]) > 100:  # Only save substantial sections
                    section_url = f"{self.base_url}#section-{i+1}"
                    section_title = section["name"]
                    section_content = section["content"]
                    
                    # Generate embedding for the section
                    section_embedding = await self._generate_embedding(section_content)
                    
                    # Prepare section metadata
                    section_metadata = {
                        "repo": self.repo_info["full_repo"],
                        "owner": self.repo_info["owner"],
                        "repo_name": self.repo_info["repo"],
                        "branch": self.repo_info["branch"],
                        "path": self.repo_info["path"],
                        "crawled_at": self._get_current_timestamp(),
                        "content
    
    def _analyze_documentation(self, markdown: str) -> Dict[str, Any]:
        """
        Analyze documentation structure from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            Dictionary with documentation structure analysis
        """
        lines = markdown.split("\n")
        sections = []
        current_section = None
        
        for i, line in enumerate(lines):
            if line.startswith("#"):
                # Count heading level
                level = 0
                for char in line:
                    if char == "#":
                        level += 1
                    else:
                        break
                
                # Extract heading text
                heading = line[level:].strip()
                
                # Close previous section if exists
                if current_section:
                    current_section["end_line"] = i - 1
                    current_section["content"] = "\n".join(lines[current_section["start_line"]:i])
                    sections.append(current_section)
                
                # Start new section
                current_section = {
                    "title": heading,
                    "level": level,
                    "start_line": i,
                    "end_line": None,
                    "content": None
                }
        
        # Close the last section
        if current_section:
            current_section["end_line"] = len(lines) - 1
            current_section["content"] = "\n".join(lines[current_section["start_line"]:])
            sections.append(current_section)
        
        return {
            "sections": sections,
            "section_count": len(sections),
            "has_introduction": any(s["title"].lower() in ["introduction", "overview", "about"] for s in sections),
            "has_installation": any(s["title"].lower() in ["installation", "setup", "getting started"] for s in sections),
            "has_api_reference": any(s["title"].lower() in ["api", "reference", "api reference"] for s in sections)
        }
    
    def _extract_api_sections(self, markdown: str) -> List[Dict[str, Any]]:
        """
        Extract API documentation sections from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            List of API sections with metadata
        """
        import re
        
        api_sections = []
        lines = markdown.split("\n")
        
        # Patterns that indicate API documentation
        api_patterns = [
            r"^\s*#{1,3}\s+.*\bAPI\b",
            r"^\s*#{1,3}\s+.*\bEndpoint\b",
            r"^\s*#{1,3}\s+.*\bMethod\b",
            r"^\s*#{1,3}\s+.*\bFunction\b",
            r"^\s*#{1,3}\s+.*\bClass\b",
            r"^\s*`[^`]+`\s*\(",  # Function signature like `function_name(`
        ]
        
        # GitHub-specific API patterns
        github_api_patterns = [
            r"^\s*#{1,3}\s+.*\bParameters\b",
            r"^\s*#{1,3}\s+.*\bReturns\b",
            r"^\s*#{1,3}\s+.*\bExamples\b",
            r"^\s*#{1,3}\s+.*\bUsage\b",
            r"^\s*#{1,3}\s+.*\bOptions\b",
        ]
        
        # Combine all patterns
        all_patterns = api_patterns + github_api_patterns
        
        # Compile patterns for efficiency
        compiled_patterns = [re.compile(pattern) for pattern in all_patterns]
        
        # Track current section
        current_section = None
        
        for i, line in enumerate(lines):
            # Check if line matches any API pattern
            for pattern in compiled_patterns:
                if pattern.search(line):
                    # Extract section name
                    section_name = line.strip("# \t")
                    
                    # If we're already in a section, close it
                    if current_section:
                        current_section["end_line"] = i - 1
                        current_section["content"] = "\n".join(lines[current_section["start_line"]:i])
                        api_sections.append(current_section)
                    
                    # Start new section
                    current_section = {
                        "name": section_name,
                        "start_line": i,
                        "end_line": None,
                        "content": None
                    }
                    break
        
        # Close the last section if there is one
        if current_section:
            current_section["end_line"] = len(lines) - 1
            current_section["content"] = "\n".join(lines[current_section["start_line"]:])
            api_sections.append(current_section)
        
        return api_sections
    
    def _extract_code_examples(self, markdown: str) -> List[Dict[str, Any]]:
        """
        Extract code examples from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            List of code examples with metadata
        """
        import re
        
        code_examples = []
        
        # Extract code blocks with their language
        code_block_pattern = r"```(\w*)\n(.*?)```"
        matches = re.findall(code_block_pattern, markdown, re.DOTALL)
        
        for i, (lang, content) in enumerate(matches):
            lang = lang.lower().strip() or "text"  # Default to "text" if no language specified
            
            # Determine purpose (heuristic)
            purpose = "other"
            content_lower = content.lower()
            if "install" in content_lower or "pip" in content_lower or "npm" in content_lower:
                purpose = "installation"
            elif "example" in content_lower or "# example" in content_lower:
                purpose = "example"
            elif "import" in content_lower and len(content.split("\n")) > 5:
                purpose = "usage"
            elif "def " in content_lower or "class " in content_lower or "function" in content_lower:
                purpose = "api"
            
            # GitHub-specific purpose detection
            if "git clone" in content_lower:
                purpose = "git_clone"
            elif "github.com" in content_lower:
                purpose = "github_reference"
            
            code_examples.append({
                "id": i + 1,
                "language": lang,
                "content": content,
                "purpose": purpose,
                "line_count": len(content.split("\n"))
            })
        
        return code_examples
    
    def _extract_github_elements(self, markdown: str) -> Dict[str, Any]:
        """
        Extract GitHub-specific elements from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            Dictionary with GitHub-specific elements
        """
        import re
        
        # Extract GitHub badges
        badge_pattern = r"!\[([^\]]*)\]\((https?://(?:img\.shields\.io|github\.com)[^\)]+)\)"
        badges = re.findall(badge_pattern, markdown)
        
        # Extract GitHub Actions references
        actions_pattern = r"github\.com/[^/]+/[^/]+/actions|github/workflows|\.github/workflows"
        has_actions = bool(re.search(actions_pattern, markdown))
        
        # Extract GitHub Issues references
        issues_pattern = r"github\.com/[^/]+/[^/]+/issues|#\d+"
        has_issues = bool(re.search(issues_pattern, markdown))
        
        # Extract GitHub Pull Requests references
        pr_pattern = r"github\.com/[^/]+/[^/]+/pull|PR"
        has_prs = bool(re.search(pr_pattern, markdown))
        
        # Extract GitHub Wiki references
        wiki_pattern = r"github\.com/[^/]+/[^/]+/wiki"
        has_wiki = bool(re.search(wiki_pattern, markdown))
        
        # Extract GitHub Releases references
        releases_pattern = r"github\.com/[^/]+/[^/]+/releases|v\d+\.\d+\.\d+"
        has_releases = bool(re.search(releases_pattern, markdown))
        
        # Extract GitHub Discussions references
        discussions_pattern = r"github\.com/[^/]+/[^/]+/discussions"
        has_discussions = bool(re.search(discussions_pattern, markdown))
        
        # Extract GitHub Sponsors references
        sponsors_pattern = r"github\.com/sponsors"
        has_sponsors = bool(re.search(sponsors_pattern, markdown))
        
        # Extract GitHub Pages references
        pages_pattern = r"github\.io"
        has_pages = bool(re.search(pages_pattern, markdown))
        
        # Extract GitHub Projects references
        projects_pattern = r"github\.com/[^/]+/[^/]+/projects"
        has_projects = bool(re.search(projects_pattern, markdown))
        
        # Extract GitHub Security references
        security_pattern = r"github\.com/[^/]+/[^/]+/security|SECURITY\.md"
        has_security = bool(re.search(security_pattern, markdown))
        
        # Extract GitHub Contributing references
        contributing_pattern = r"CONTRIBUTING\.md"
        has_contributing = bool(re.search(contributing_pattern, markdown))
        
        # Extract GitHub Code of Conduct references
        coc_pattern = r"CODE_OF_CONDUCT\.md"
        has_coc = bool(re.search(coc_pattern, markdown))
        
        # Extract GitHub License references
        license_pattern = r"LICENSE|license\."
        has_license = bool(re.search(license_pattern, markdown))
        
        return {
            "badges": badges,
            "has_actions": has_actions,
            "has_issues": has_issues,
            "has_prs": has_prs,
            "has_wiki": has_wiki,
            "has_releases": has_releases,
            "has_discussions": has_discussions,
            "has_sponsors": has_sponsors,
            "has_pages": has_pages,
            "has_projects": has_projects,
            "has_security": has_security,
            "has_contributing": has_contributing,
            "has_coc": has_coc,
            "has_license": has_license
        }
    
    def _count_languages(self, code_examples: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        Count code examples by language.
        
        Args:
            code_examples: List of code examples
            
        Returns:
            Dictionary mapping languages to counts
        """
        language_counts = {}
        for example in code_examples:
            lang = example["language"]
            language_counts[lang] = language_counts.get(lang, 0) + 1
        
        return language_counts
    
    async def _save_documentation(
        self, 
        result: Any, 
        doc_structure: Dict[str, Any],
        api_sections: List[Dict[str, Any]],
        code_examples: List[Dict[str, Any]]
    ) -> None:
        """
        Save extracted documentation to files.
        
        Args:
            result: Crawler result
            doc_structure: Documentation structure analysis
            api_sections: Extracted API sections
            code_examples: Extracted code examples
        """
        # Save full markdown content
        with open(os.path.join(self.output_dir, "full_documentation.md"), "w", encoding="utf-8") as f:
            f.write(result.markdown.raw_markdown)
        
        # Save documentation metrics
        with open(os.path.join(self.output_dir, "doc_metrics.json"), "w", encoding="utf-8") as f:
            json.dump(self.doc_metrics, f, indent=2)
        
        # Save API sections to separate files
        for i, section in enumerate(api_sections):
            filename = f"api_{i+1}_{self._sanitize_filename(section['name'])}.md"
            with open(os.path.join(self.api_dir, filename), "w", encoding="utf-8") as f:
                f.write(section["content"])
        
        # Save code examples to separate files
        for example in code_examples:
            ext = example["language"] if example["language"] != "text" else "txt"
            filename = f"example_{example['id']}_{example['purpose']}.{ext}"
            with open(os.path.join(self.code_dir, filename), "w", encoding="utf-8") as f:
                f.write(example["content"])
        
        # Save README if this is a repository root
        if not self.repo_info["path"] and doc_structure["sections"]:
            with open(os.path.join(self.readme_dir, "README.md"), "w", encoding="utf-8") as f:
                f.write(result.markdown.raw_markdown)
    
    async def _save_to_supabase(
        self,
        result: Any,
        doc_structure: Dict[str, Any],
        api_sections: List[Dict[str, Any]],
        code_examples: List[Dict[str, Any]]
    ) -> None:
        """
        Save documentation to Supabase.
        
        Args:
            result: Crawler result
            doc_structure: Documentation structure analysis
            api_sections: Extracted API sections
            code_examples: Extracted code examples
        """
        try:
            # Generate embedding for the content
            embedding = await self._generate_embedding(result.markdown.raw_markdown)
            
            # Prepare metadata
            metadata = {
                "repo": self.repo_info["full_repo"],
                "owner": self.repo_info["owner"],
                "repo_name": self.repo_info["repo"],
                "branch": self.repo_info["branch"],
                "path": self.repo_info["path"],
                "crawled_at": self._get_current_timestamp(),
                "content_length": len(result.markdown.raw_markdown),
                "doc_metrics": self.doc_metrics,
                "api_sections_count": len(api_sections),
                "code_examples_count": len(code_examples)
            }
            
            # Prepare data for insertion
            data = {
                "url": self.base_url,
                "title": result.title or f"Documentation for {self.repo_info['full_repo']}",
                "content": result.markdown.raw_markdown,
                "metadata": metadata,
                "embedding": embedding
            }
            
            # Insert or update the page
            self.supabase_client.table("crawl4ai_site_pages").upsert(data).execute()
            
            logger.info(f"Saved documentation to Supabase: {self.base_url}")
            
            # Save API sections as separate entries if they're substantial
            for i, section in enumerate(api_sections):
                if len(section["content"]) > 100:  # Only save substantial sections
                    section_url = f"{self.base_url}#section-{i+1}"
                    section_title = section["name"]
                    section_content = section["content"]
                    
                    # Generate embedding for the section
                    section_embedding = await self._generate_embedding(section_content)
                    
                    # Prepare section metadata
                    section_metadata = {
                        "repo": self.repo_info["full_repo"],
                        "owner": self.repo_info["owner"],
                        "repo_name": self.repo_info["repo"],
                        "branch": self.repo_info["branch"],
                        "path": self.repo_info["path"],
                        "crawled_at": self._get_current_timestamp(),
                        "content_length": len(section_content),
                        "section_type": "api",
                        "parent_url": self.base_url
                    }
                    
                    # Prepare data for insertion
                    section_data = {
                        "url": section_url,
                        "title": section_title,
                        "content": section_content,
                        "metadata": section_metadata,
                        "embedding": section_embedding
                    }
                    
                    # Insert or update the section
                    self.supabase_client.table("crawl4ai_site_pages").upsert(section_data).execute()
                    
                    logger.info(f"Saved API section to Supabase: {section_title}")
            
        except Exception as e:
            logger.error(f"Error saving to Supabase: {str(e)}")
    
    async def _generate_embedding(self, text: str) -> List[float]:
        """
        Generate an embedding for the given text.
        
        Args:
            text: The text to generate an embedding for
            
        Returns:
            The embedding vector
        """
        try:
            # This is a placeholder. In a real implementation, you would use
            # an embedding service like OpenAI's API to generate embeddings.
            # For now, we'll return a dummy embedding.
            return [0.0] * 1536  # OpenAI embeddings are 1536-dimensional
        except Exception as e:
            logger.error(f"Error generating embedding: {str(e)}")
            return [0.0] * 1536
    
    def _get_current_timestamp(self) -> str:
        """
        Get the current timestamp in ISO format.
        
        Returns:
            Current timestamp
        """
        from datetime import datetime, timezone
        return datetime.now(timezone.utc).isoformat()
    
    def _sanitize_filename(self, filename: str) -> str:
        """
        Sanitize a filename to ensure it's valid across operating systems.
        
        Args:
            filename: The filename to sanitize
            
        Returns:
            Sanitized filename
        """
        import re
        
        # Replace invalid characters with underscores
        sanitized = re.sub(r'[\\/*?:"<>|]', '_', filename)
        # Replace multiple underscores with a single one
        sanitized = re.sub(r'_+', '_', sanitized)
        # Remove leading/trailing underscores
        sanitized = sanitized.strip('_')
        # Ensure the filename is not empty
        if not sanitized:
            sanitized = "unnamed"
        
        return sanitized

async def main():
    """
    Main entry point for the GitHub documentation scraper.
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="GitHub Documentation Scraper")
    parser.add_argument("--url", required=True, help="GitHub repository URL")
    parser.add_argument("--output-dir", default="./output", help="Output directory")
    parser.add_argument("--supabase-url", help="Supabase URL")
    parser.add_argument("--supabase-key", help="Supabase key")
    
    args = parser.parse_args()
    
    scraper = GitHubDocumentationScraper(
        output_dir=args.output_dir,
        supabase_url=args.supabase_url,
        supabase_key=args.supabase_key
    )
    
    result = await scraper.scrape_documentation()
    
    if "error" in result:
        logger.error(f"Error: {result['error']}")
        return
    
    logger.info(f"Documentation scraped successfully from {result['url']}")
    logger.info(f"Title: {result['title']}")
    logger.info(f"Found {result['structure']['section_count']} sections")
    logger.info(f"Found {len(result['api_sections'])} API sections")
    logger.info(f"Found {len(result['code_examples'])} code examples")
    logger.info(f"Output saved to {result['output_dir']}")

# Run the scraper
if __name__ == "__main__":
    asyncio.run(main())
