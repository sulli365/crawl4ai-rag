"""
GitHub Documentation scraper for {{ url }}
Purpose: {{ purpose }}
Generated by crawl4ai-rag
"""

import asyncio
import json
import os
import re
from typing import Dict, List, Any, Optional, Tuple
from urllib.parse import urljoin, urlparse

from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from loguru import logger

# Import Supabase integration
from supabase import create_client, Client

class GitHubDocumentationScraper:
    """
    Specialized scraper for GitHub documentation at {{ url }}.
    Purpose: {{ purpose }}
    """
    
    def __init__(
        self, 
        output_dir: str = "./output",
        supabase_url: Optional[str] = None,
        supabase_key: Optional[str] = None
    ):
        """
        Initialize the GitHub documentation scraper.
        
        Args:
            output_dir: Directory to save extracted documentation
            supabase_url: Supabase URL (if None, will use environment variable)
            supabase_key: Supabase key (if None, will use environment variable)
        """
        self.output_dir = output_dir
        self.base_url = "{{ url }}"
        self.repo_info = self._parse_github_url(self.base_url)
        self.doc_metrics = {}
        
        # Set up Supabase client
        self.supabase_url = supabase_url or os.environ.get("SUPABASE_URL")
        self.supabase_key = supabase_key or os.environ.get("SUPABASE_KEY")
        self.supabase_client = None
        
        if self.supabase_url and self.supabase_key:
            try:
                self.supabase_client = create_client(self.supabase_url, self.supabase_key)
                logger.info("Supabase client initialized successfully")
            except Exception as e:
                logger.error(f"Failed to initialize Supabase client: {str(e)}")
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Create subdirectories for different content types
        self.api_dir = os.path.join(output_dir, "api")
        self.code_dir = os.path.join(output_dir, "code_examples")
        self.readme_dir = os.path.join(output_dir, "readme")
        
        os.makedirs(self.api_dir, exist_ok=True)
        os.makedirs(self.code_dir, exist_ok=True)
        os.makedirs(self.readme_dir, exist_ok=True)
    
    def _parse_github_url(self, url: str) -> Dict[str, str]:
        """
        Parse a GitHub URL to extract repository information.
        
        Args:
            url: GitHub URL
            
        Returns:
            Dictionary with repository information
        """
        parsed = urlparse(url)
        path_parts = parsed.path.strip("/").split("/")
        
        if len(path_parts) < 2:
            raise ValueError(f"Invalid GitHub URL: {url}")
        
        owner = path_parts[0]
        repo = path_parts[1]
        
        # Determine if this is a specific path within the repo
        path = "/".join(path_parts[2:]) if len(path_parts) > 2 else ""
        
        # Determine if this is a specific branch
        branch = "main"  # Default branch
        if len(path_parts) > 4 and path_parts[2] == "tree":
            branch = path_parts[3]
            path = "/".join(path_parts[4:])
        
        return {
            "owner": owner,
            "repo": repo,
            "branch": branch,
            "path": path,
            "full_repo": f"{owner}/{repo}"
        }
    
    async def scrape_documentation(self) -> Dict[str, Any]:
        """
        Scrape the GitHub documentation and extract structured content.
        
        Returns:
            Dictionary with extracted documentation data
        """
        # Configure browser
        browser_config = BrowserConfig(
            headless=True,
            verbose=False,
            extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
        )
        
        # Configure crawler with GitHub-specific settings
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            excluded_tags=['form', 'header', 'footer', 'nav'],
            exclude_external_links=True,
            process_iframes=True,
            remove_overlay_elements=True,
        )
        
        try:
            # Create the crawler instance
            async with AsyncWebCrawler(config=browser_config) as crawler:
                # Crawl the URL
                result = await crawler.arun(
                    url=self.base_url,
                    config=crawl_config
                )
                
                if not result.success:
                    logger.error(f"Failed to crawl: {result.error_message}")
                    return {"error": result.error_message}
                
                # Extract documentation structure
                doc_structure = self._analyze_documentation(result.markdown_v2.raw_markdown)
                
                # Extract API sections
                api_sections = self._extract_api_sections(result.markdown_v2.raw_markdown)
                
                # Extract code examples
                code_examples = self._extract_code_examples(result.markdown_v2.raw_markdown)
                
                # Extract GitHub-specific elements
                github_elements = self._extract_github_elements(result.markdown_v2.raw_markdown)
                
                # Save documentation metrics
                self.doc_metrics = {
                    "url": self.base_url,
                    "title": result.title,
                    "repo": self.repo_info["full_repo"],
                    "branch": self.repo_info["branch"],
                    "path": self.repo_info["path"],
                    "sections": doc_structure["sections"],
                    "code_examples": {
                        "count": len(code_examples),
                        "languages": self._count_languages(code_examples)
                    },
                    "api_sections": len(api_sections),
                    "github_elements": github_elements,
                    "internal_links": len(result.links["internal"]),
                    "external_links": len(result.links["external"])
                }
                
                # Save documentation to files
                await self._save_documentation(result, doc_structure, api_sections, code_examples)
                
                # Save to Supabase if client is available
                if self.supabase_client:
                    await self._save_to_supabase(result, doc_structure, api_sections, code_examples)
                
                return {
                    "title": result.title,
                    "url": self.base_url,
                    "repo": self.repo_info["full_repo"],
                    "structure": doc_structure,
                    "api_sections": api_sections,
                    "code_examples": code_examples,
                    "github_elements": github_elements,
                    "output_dir": self.output_dir
                }
                
        except Exception as e:
            logger.error(f"Error scraping documentation: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_documentation(self, markdown: str) -> Dict[str, Any]:
        """
        Analyze documentation structure from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            Dictionary with documentation structure analysis
        """
        lines = markdown.split("\n")
        sections = []
        current_section = None
        
        for i, line in enumerate(lines):
            if line.startswith("#"):
                # Count heading level
                level = 0
                for char in line:
                    if char == "#":
                        level += 1
                    else:
                        break
                
                # Extract heading text
                heading = line[level:].strip()
                
                # Close previous section if exists
                if current_section:
                    current_section["end_line"] = i - 1
                    current_section["content"] = "\n".join(lines[current_section["start_line"]:i])
                    sections.append(current_section)
                
                # Start new section
                current_section = {
                    "title": heading,
                    "level": level,
                    "start_line": i,
                    "end_line": None,
                    "content": None
                }
        
        # Close the last section
        if current_section:
            current_section["end_line"] = len(lines) - 1
            current_section["content"] = "\n".join(lines[current_section["start_line"]:])
            sections.append(current_section)
        
        return {
            "sections": sections,
            "section_count": len(sections),
            "has_introduction": any(s["title"].lower() in ["introduction", "overview", "about"] for s in sections),
            "has_installation": any(s["title"].lower() in ["installation", "setup", "getting started"] for s in sections),
            "has_api_reference": any(s["title"].lower() in ["api", "reference", "api reference"] for s in sections)
        }
    
    def _extract_api_sections(self, markdown: str) -> List[Dict[str, Any]]:
        """
        Extract API documentation sections from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            List of API sections with metadata
        """
        import re
        
        api_sections = []
        lines = markdown.split("\n")
        
        # Patterns that indicate API documentation
        api_patterns = [
            r"^\s*#{1,3}\s+.*\bAPI\b",
            r"^\s*#{1,3}\s+.*\bEndpoint\b",
            r"^\s*#{1,3}\s+.*\bMethod\b",
            r"^\s*#{1,3}\s+.*\bFunction\b",
            r"^\s*#{1,3}\s+.*\bClass\b",
            r"^\s*`[^`]+`\s*\(",  # Function signature like `function_name(`
        ]
        
        # GitHub-specific API patterns
        github_api_patterns = [
            r"^\s*#{1,3}\s+.*\bParameters\b",
            r"^\s*#{1,3}\s+.*\bReturns\b",
            r"^\s*#{1,3}\s+.*\bExamples\b",
            r"^\s*#{1,3}\s+.*\bUsage\b",
            r"^\s*#{1,3}\s+.*\bOptions\b",
        ]
        
        # Combine all patterns
        all_patterns = api_patterns + github_api_patterns
        
        # Compile patterns for efficiency
        compiled_patterns = [re.compile(pattern) for pattern in all_patterns]
        
        # Track current section
        current_section = None
        
        for i, line in enumerate(lines):
            # Check if line matches any API pattern
            for pattern in compiled_patterns:
                if pattern.search(line):
                    # Extract section name
                    section_name = line.strip("# \t")
                    
                    # If we're already in a section, close it
                    if current_section:
                        current_section["end_line"] = i - 1
                        current_section["content"] = "\n".join(lines[current_section["start_line"]:i])
                        api_sections.append(current_section)
                    
                    # Start new section
                    current_section = {
                        "name": section_name,
                        "start_line": i,
                        "end_line": None,
                        "content": None
                    }
                    break
        
        # Close the last section if there is one
        if current_section:
            current_section["end_line"] = len(lines) - 1
            current_section["content"] = "\n".join(lines[current_section["start_line"]:])
            api_sections.append(current_section)
        
        return api_sections
    
    def _extract_code_examples(self, markdown: str) -> List[Dict[str, Any]]:
        """
        Extract code examples from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            List of code examples with metadata
        """
        import re
        
        code_examples = []
        
        # Extract code blocks with their language
        code_block_pattern = r"```(\w*)\n(.*?)```"
        matches = re.findall(code_block_pattern, markdown, re.DOTALL)
        
        for i, (lang, content) in enumerate(matches):
            lang = lang.lower().strip() or "text"  # Default to "text" if no language specified
            
            # Determine purpose (heuristic)
            purpose = "other"
            content_lower = content.lower()
            if "install" in content_lower or "pip" in content_lower or "npm" in content_lower:
                purpose = "installation"
            elif "example" in content_lower or "# example" in content_lower:
                purpose = "example"
            elif "import" in content_lower and len(content.split("\n")) > 5:
                purpose = "usage"
            elif "def " in content_lower or "class " in content_lower or "function" in content_lower:
                purpose = "api"
            
            # GitHub-specific purpose detection
            if "git clone" in content_lower:
                purpose = "git_clone"
            elif "github.com" in content_lower:
                purpose = "github_reference"
            
            code_examples.append({
                "id": i + 1,
                "language": lang,
                "content": content,
                "purpose": purpose,
                "line_count": len(content.split("\n"))
            })
        
        return code_examples
    
    def _extract_github_elements(self, markdown: str) -> Dict[str, Any]:
        """
        Extract GitHub-specific elements from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            Dictionary with GitHub-specific elements
        """
        import re
        
        # Extract GitHub badges
        badge_pattern = r"!\[([^\]]*)\]\((https?://(?:img\.shields\.io|github\.com)[^\)]+)\)"
        badges = re.findall(badge_pattern, markdown)
        
        # Extract GitHub Actions references
        actions_pattern = r"github\.com/[^/]+/[^/]+/actions|github/workflows|\.github/workflows"
        has_actions = bool(re.search(actions_pattern, markdown))
        
        # Extract GitHub Issues references
        issues_pattern = r"github\.com/[^/]+/[^/]+/issues|#\d+"
        has_issues = bool(re.search(issues_pattern, markdown))
        
        # Extract GitHub Pull Requests references
        pr_pattern = r"github\.com/[^/]+/[^/]+/pull|PR"
        has_prs = bool(re.search(pr_pattern, markdown))
        
        # Extract GitHub Wiki references
        wiki_pattern = r"github\.com/[^/]+/[^/]+/wiki"
        has_wiki = bool(re.search(wiki_pattern, markdown))
        
        # Extract GitHub Releases references
        releases_pattern = r"github\.com/[^/]+/[^/]+/releases|v\d+\.\d+\.\d+"
        has_releases = bool(re.search(releases_pattern, markdown))
        
        # Extract GitHub Discussions references
        discussions_pattern = r"github\.com/[^/]+/[^/]+/discussions"
        has_discussions = bool(re.search(discussions_pattern, markdown))
        
        # Extract GitHub Sponsors references
        sponsors_pattern = r"github\.com/sponsors"
        has_sponsors = bool(re.search(sponsors_pattern, markdown))
        
        # Extract GitHub Pages references
        pages_pattern = r"github\.io"
        has_pages = bool(re.search(pages_pattern, markdown))
        
        # Extract GitHub Projects references
        projects_pattern = r"github\.com/[^/]+/[^/]+/projects"
        has_projects = bool(re.search(projects_pattern, markdown))
        
        # Extract GitHub Security references
        security_pattern = r"github\.com/[^/]+/[^/]+/security|SECURITY\.md"
        has_security = bool(re.search(security_pattern, markdown))
        
        # Extract GitHub Contributing references
        contributing_pattern = r"CONTRIBUTING\.md"
        has_contributing = bool(re.search(contributing_pattern, markdown))
        
        # Extract GitHub Code of Conduct references
        coc_pattern = r"CODE_OF_CONDUCT\.md"
        has_coc = bool(re.search(coc_pattern, markdown))
        
        # Extract GitHub License references
        license_pattern = r"LICENSE|license\."
        has_license = bool(re.search(license_pattern, markdown))
        
        return {
            "badges": badges,
            "has_actions": has_actions,
            "has_issues": has_issues,
            "has_prs": has_prs,
            "has_wiki": has_wiki,
            "has_releases": has_releases,
            "has_discussions": has_discussions,
            "has_sponsors": has_sponsors,
            "has_pages": has_pages,
            "has_projects": has_projects,
            "has_security": has_security,
            "has_contributing": has_contributing,
            "has_coc": has_coc,
            "has_license": has_license
        }
    
    def _count_languages(self, code_examples: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        Count code examples by language.
        
        Args:
            code_examples: List of code examples
            
        Returns:
            Dictionary mapping languages to counts
        """
        language_counts = {}
        for example in code_examples:
            lang = example["language"]
            language_counts[lang] = language_counts.get(lang, 0) + 1
        
        return language_counts
    
    async def _save_documentation(
        self, 
        result: Any, 
        doc_structure: Dict[str, Any],
        api_sections: List[Dict[str, Any]],
        code_examples: List[Dict[str, Any]]
    ) -> None:
        """
        Save extracted documentation to files.
        
        Args:
            result: Crawler result
            doc_structure: Documentation structure analysis
            api_sections: Extracted API sections
            code_examples: Extracted code examples
        """
        # Save full markdown content
        with open(os.path.join(self.output_dir, "full_documentation.md"), "w", encoding="utf-8") as f:
            f.write(result.markdown_v2.raw_markdown)
        
        # Save documentation metrics
        with open(os.path.join(self.output_dir, "doc_metrics.json"), "w", encoding="utf-8") as f:
            json.dump(self.doc_metrics, f, indent=2)
        
        # Save API sections to separate files
        for i, section in enumerate(api_sections):
            filename = f"api_{i+1}_{self._sanitize_filename(section['name'])}.md"
            with open(os.path.join(self.api_dir, filename), "w", encoding="utf-8") as f:
                f.write(section["content"])
        
        # Save code examples to separate files
        for example in code_examples:
            ext = example["language"] if example["language"] != "text" else "txt"
            filename = f"example_{example['id']}_{example['purpose']}.{ext}"
            with open(os.path.join(self.code_dir, filename), "w", encoding="utf-8") as f:
                f.write(example["content"])
        
        # Save README if this is a repository root
        if not self.repo_info["path"] and doc_structure["sections"]:
            with open(os.path.join(self.readme_dir, "README.md"), "w", encoding="utf-8") as f:
                f.write(result.markdown_v2.raw_markdown)
    
    async def _save_to_supabase(
        self,
        result: Any,
        doc_structure: Dict[str, Any],
        api_sections: List[Dict[str, Any]],
        code_examples: List[Dict[str, Any]]
    ) -> None:
        """
        Save documentation to Supabase.
        
        Args:
            result: Crawler result
            doc_structure: Documentation structure analysis
            api_sections: Extracted API sections
            code_examples: Extracted code examples
        """
        try:
            # Generate embedding for the content
            embedding = await self._generate_embedding(result.markdown_v2.raw_markdown)
            
            # Prepare metadata
            metadata = {
                "repo": self.repo_info["full_repo"],
                "owner": self.repo_info["owner"],
                "repo_name": self.repo_info["repo"],
                "branch": self.repo_info["branch"],
                "path": self.repo_info["path"],
                "crawled_at": self._get_current_timestamp(),
                "content_length": len(result.markdown_v2.raw_markdown),
                "doc_metrics": self.doc_metrics,
                "api_sections_count": len(api_sections),
                "code_examples_count": len(code_examples)
            }
            
            # Prepare data for insertion
            data = {
                "url": self.base_url,
                "title": result.title or f"Documentation for {self.repo_info['full_repo']}",
                "content": result.markdown_v2.raw_markdown,
                "metadata": metadata,
                "embedding": embedding
            }
            
            # Insert or update the page
            self.supabase_client.table("crawl4ai_site_pages").upsert(data).execute()
            
            logger.info(f"Saved documentation to Supabase: {self.base_url}")
            
            # Save API sections as separate entries if they're substantial
            for i, section in enumerate(api_sections):
                if len(section["content"]) > 100:  # Only save substantial sections
                    section_url = f"{self.base_url}#section-{i+1}"
                    section_title = section["name"]
                    section_content = section["content"]
                    
                    # Generate embedding for the section
                    section_embedding = await self._generate_embedding(section_content)
                    
                    # Prepare section metadata
                    section_metadata = {
                        "repo": self.repo_info["full_repo"],
                        "owner": self.repo_info["owner"],
                        "repo_name": self.repo_info["repo"],
                        "branch": self.repo_info["branch"],
                        "path": self.repo_info["path"],
                        "crawled_at": self._get_current_timestamp(),
                        "content_length": len(section_content),
                        "section_type": "api",
                        "parent_url": self.base_url
                    }
                    
                    # Prepare data for insertion
                    section_data = {
                        "url": section_url,
                        "title": section_title,
                        "content": section_content,
                        "metadata": section_metadata,
                        "embedding": section_embedding
                    }
                    
                    # Insert or update the section
                    self.supabase_client.table("crawl4ai_site_pages").upsert(section_data).execute()
                    
                    logger.info(f"Saved API section to Supabase: {section_title}")
            
        except Exception as e:
            logger.error(f"Error saving to Supabase: {str(e)}")
    
    async def _generate_embedding(self, text: str) -> List[float]:
        """
        Generate an embedding for the given text.
        
        Args:
            text: The text to generate an embedding for
            
        Returns:
            The embedding vector
        """
        try:
            # This is a placeholder. In a real implementation, you would use
            # an embedding service like OpenAI's API to generate embeddings.
            # For now, we'll return a dummy embedding.
            return [0.0] * 1536  # OpenAI embeddings are 1536-dimensional
        except Exception as e:
            logger.error(f"Error generating embedding: {str(e)}")
            return [0.0] * 1536
    
    def _get_current_timestamp(self) -> str:
        """
        Get the current timestamp in ISO format.
        
        Returns:
            Current timestamp
        """
        from datetime import datetime, timezone
        return datetime.now(timezone.utc).isoformat()
    
    def _sanitize_filename(self, filename: str) -> str:
        """
        Sanitize a filename to ensure it's valid across operating systems.
        
        Args:
            filename: The filename to sanitize
            
        Returns:
            Sanitized filename
        """
        import re
        
        # Replace invalid characters with underscores
        sanitized = re.sub(r'[\\/*?:"<>|]', '_', filename)
        # Replace multiple underscores with a single one
        sanitized = re.sub(r'_+', '_', sanitized)
        # Remove leading/trailing underscores
        sanitized = sanitized.strip('_')
        # Ensure the filename is not empty
        if not sanitized:
            sanitized = "unnamed"
        
        return sanitized

async def main():
    """
    Main entry point for the GitHub documentation scraper.
    """
    import argparse
    
    parser = argparse.ArgumentParser(description="GitHub Documentation Scraper")
    parser.add_argument("--url", required=True, help="GitHub repository URL")
    parser.add_argument("--output-dir", default="./output", help="Output directory")
    parser.add_argument("--supabase-url", help="Supabase URL")
    parser.add_argument("--supabase-key", help="Supabase key")
    
    args = parser.parse_args()
    
    scraper = GitHubDocumentationScraper(
        output_dir=args.output_dir,
        supabase_url=args.supabase_url,
        supabase_key=args.supabase_key
    )
    
    result = await scraper.scrape_documentation()
    
    if "error" in result:
        logger.error(f"Error: {result['error']}")
        return
    
    logger.info(f"Documentation scraped successfully from {result['url']}")
    logger.info(f"Title: {result['title']}")
    logger.info(f"Found {result['structure']['section_count']} sections")
    logger.info(f"Found {len(result['api_sections'])} API sections")
    logger.info(f"Found {len(result['code_examples'])} code examples")
    logger.info(f"Output saved to {result['output_dir']}")

# Run the scraper
if __name__ == "__main__":
    asyncio.run(main())
