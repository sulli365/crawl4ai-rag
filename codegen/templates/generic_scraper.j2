"""
Generic web scraper for {{ url }}
Purpose: {{ purpose }}
Generated by crawl4ai-rag
"""

import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def scrape_website():
    """
    Scrape the website at {{ url }} for {{ purpose }}.
    """
    # Configure browser
    browser_config = BrowserConfig(
        headless=True,
        verbose=False,
        extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
    )
    
    # Configure crawler
    crawl_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        excluded_tags=['form', 'header'],
        exclude_external_links=True,
        process_iframes=True,
        remove_overlay_elements=True,
    )
    
    # Create the crawler instance
    async with AsyncWebCrawler(config=browser_config) as crawler:
        # Crawl the URL
        result = await crawler.arun(
            url="{{ url }}",
            config=crawl_config
        )
        
        if not result.success:
            print(f"Failed to crawl: {result.error_message}")
            return
        
        # Print the title
        print(f"Title: {result.title}")
        
        # Print the first 500 characters of the markdown content
        print("\nContent (first 500 chars):")
        print("-" * 50)
        print(result.markdown_v2.raw_markdown[:500])
        print("-" * 50)
        
        # Print internal links
        print("\nInternal links found:")
        for link in result.links["internal"]:
            print(f"- {link['href']}")
        
        # Return the result for further processing
        return result

# Run the crawler
if __name__ == "__main__":
    asyncio.run(scrape_website())
