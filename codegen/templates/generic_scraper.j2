"""
Documentation scraper for {{ url }}
Purpose: {{ purpose }}
Generated by crawl4ai-rag
"""

import asyncio
import json
import os
from typing import Dict, List, Any, Optional
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

class DocumentationScraper:
    """
    Specialized scraper for documentation websites at {{ url }}.
    Purpose: {{ purpose }}
    """
    
    def __init__(self, output_dir: str = "./output"):
        """
        Initialize the documentation scraper.
        
        Args:
            output_dir: Directory to save extracted documentation
        """
        self.output_dir = output_dir
        self.base_url = "{{ url }}"
        self.doc_metrics = {}
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
    
    async def scrape_documentation(self) -> Dict[str, Any]:
        """
        Scrape the documentation website and extract structured content.
        
        Returns:
            Dictionary with extracted documentation data
        """
        # Configure browser
        browser_config = BrowserConfig(
            headless=True,
            verbose=False,
            extra_args=["--disable-gpu", "--disable-dev-shm-usage", "--no-sandbox"],
        )
        
        # Configure crawler with documentation-specific settings
        crawl_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            excluded_tags=['form', 'header', 'footer', 'nav'],
            exclude_external_links=True,
            process_iframes=True,
            remove_overlay_elements=True,
        )
        
        try:
            # Create the crawler instance
            async with AsyncWebCrawler(config=browser_config) as crawler:
                # Crawl the URL
                result = await crawler.arun(
                    url=self.base_url,
                    config=crawl_config
                )
                
                if not result.success:
                    print(f"Failed to crawl: {result.error_message}")
                    return {"error": result.error_message}
                
                # Extract documentation structure
                doc_structure = self._analyze_documentation(result.markdown_v2.raw_markdown)
                
                # Extract API sections
                api_sections = self._extract_api_sections(result.markdown_v2.raw_markdown)
                
                # Extract code examples
                code_examples = self._extract_code_examples(result.markdown_v2.raw_markdown)
                
                # Save documentation metrics
                self.doc_metrics = {
                    "url": self.base_url,
                    "title": result.title,
                    "sections": doc_structure["sections"],
                    "code_examples": {
                        "count": len(code_examples),
                        "languages": self._count_languages(code_examples)
                    },
                    "api_sections": len(api_sections),
                    "internal_links": len(result.links["internal"]),
                    "external_links": len(result.links["external"])
                }
                
                # Save documentation to files
                await self._save_documentation(result, doc_structure, api_sections, code_examples)
                
                return {
                    "title": result.title,
                    "url": self.base_url,
                    "structure": doc_structure,
                    "api_sections": api_sections,
                    "code_examples": code_examples,
                    "output_dir": self.output_dir
                }
                
        except Exception as e:
            print(f"Error scraping documentation: {str(e)}")
            return {"error": str(e)}
    
    def _analyze_documentation(self, markdown: str) -> Dict[str, Any]:
        """
        Analyze documentation structure from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            Dictionary with documentation structure analysis
        """
        lines = markdown.split("\n")
        sections = []
        current_section = None
        
        for i, line in enumerate(lines):
            if line.startswith("#"):
                # Count heading level
                level = 0
                for char in line:
                    if char == "#":
                        level += 1
                    else:
                        break
                
                # Extract heading text
                heading = line[level:].strip()
                
                # Close previous section if exists
                if current_section:
                    current_section["end_line"] = i - 1
                    current_section["content"] = "\n".join(lines[current_section["start_line"]:i])
                    sections.append(current_section)
                
                # Start new section
                current_section = {
                    "title": heading,
                    "level": level,
                    "start_line": i,
                    "end_line": None,
                    "content": None
                }
        
        # Close the last section
        if current_section:
            current_section["end_line"] = len(lines) - 1
            current_section["content"] = "\n".join(lines[current_section["start_line"]:])
            sections.append(current_section)
        
        return {
            "sections": sections,
            "section_count": len(sections),
            "has_introduction": any(s["title"].lower() in ["introduction", "overview", "about"] for s in sections),
            "has_installation": any(s["title"].lower() in ["installation", "setup", "getting started"] for s in sections),
            "has_api_reference": any(s["title"].lower() in ["api", "reference", "api reference"] for s in sections)
        }
    
    def _extract_api_sections(self, markdown: str) -> List[Dict[str, Any]]:
        """
        Extract API documentation sections from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            List of API sections with metadata
        """
        import re
        
        api_sections = []
        lines = markdown.split("\n")
        
        # Patterns that indicate API documentation
        api_patterns = [
            r"^\s*#{1,3}\s+.*\bAPI\b",
            r"^\s*#{1,3}\s+.*\bEndpoint\b",
            r"^\s*#{1,3}\s+.*\bMethod\b",
            r"^\s*#{1,3}\s+.*\bFunction\b",
            r"^\s*#{1,3}\s+.*\bClass\b",
            r"^\s*`[^`]+`\s*\(",  # Function signature like `function_name(`
        ]
        
        # Compile patterns for efficiency
        compiled_patterns = [re.compile(pattern) for pattern in api_patterns]
        
        # Track current section
        current_section = None
        
        for i, line in enumerate(lines):
            # Check if line matches any API pattern
            for pattern in compiled_patterns:
                if pattern.search(line):
                    # Extract section name
                    section_name = line.strip("# \t")
                    
                    # If we're already in a section, close it
                    if current_section:
                        current_section["end_line"] = i - 1
                        current_section["content"] = "\n".join(lines[current_section["start_line"]:i])
                        api_sections.append(current_section)
                    
                    # Start new section
                    current_section = {
                        "name": section_name,
                        "start_line": i,
                        "end_line": None,
                        "content": None
                    }
                    break
        
        # Close the last section if there is one
        if current_section:
            current_section["end_line"] = len(lines) - 1
            current_section["content"] = "\n".join(lines[current_section["start_line"]:])
            api_sections.append(current_section)
        
        return api_sections
    
    def _extract_code_examples(self, markdown: str) -> List[Dict[str, Any]]:
        """
        Extract code examples from markdown content.
        
        Args:
            markdown: Raw markdown content
            
        Returns:
            List of code examples with metadata
        """
        import re
        
        code_examples = []
        
        # Extract code blocks with their language
        code_block_pattern = r"```(\w*)\n(.*?)```"
        matches = re.findall(code_block_pattern, markdown, re.DOTALL)
        
        for i, (lang, content) in enumerate(matches):
            lang = lang.lower().strip() or "text"  # Default to "text" if no language specified
            
            # Determine purpose (heuristic)
            purpose = "other"
            content_lower = content.lower()
            if "install" in content_lower or "pip" in content_lower or "npm" in content_lower:
                purpose = "installation"
            elif "example" in content_lower or "# example" in content_lower:
                purpose = "example"
            elif "import" in content_lower and len(content.split("\n")) > 5:
                purpose = "usage"
            elif "def " in content_lower or "class " in content_lower or "function" in content_lower:
                purpose = "api"
            
            code_examples.append({
                "id": i + 1,
                "language": lang,
                "content": content,
                "purpose": purpose,
                "line_count": len(content.split("\n"))
            })
        
        return code_examples
    
    def _count_languages(self, code_examples: List[Dict[str, Any]]) -> Dict[str, int]:
        """
        Count code examples by language.
        
        Args:
            code_examples: List of code examples
            
        Returns:
            Dictionary mapping languages to counts
        """
        language_counts = {}
        for example in code_examples:
            lang = example["language"]
            language_counts[lang] = language_counts.get(lang, 0) + 1
        
        return language_counts
    
    async def _save_documentation(
        self, 
        result: Any, 
        doc_structure: Dict[str, Any],
        api_sections: List[Dict[str, Any]],
        code_examples: List[Dict[str, Any]]
    ) -> None:
        """
        Save extracted documentation to files.
        
        Args:
            result: Crawler result
            doc_structure: Documentation structure analysis
            api_sections: Extracted API sections
            code_examples: Extracted code examples
        """
        # Save full markdown content
        with open(os.path.join(self.output_dir, "full_documentation.md"), "w", encoding="utf-8") as f:
            f.write(result.markdown_v2.raw_markdown)
        
        # Save documentation metrics
        with open(os.path.join(self.output_dir, "doc_metrics.json"), "w", encoding="utf-8") as f:
            json.dump(self.doc_metrics, f, indent=2)
        
        # Save API sections to separate files
        api_dir = os.path.join(self.output_dir, "api")
        os.makedirs(api_dir, exist_ok=True)
        
        for i, section in enumerate(api_sections):
            filename = f"api_{i+1}_{self._sanitize_filename(section['name'])}.md"
            with open(os.path.join(api_dir, filename), "w", encoding="utf-8") as f:
                f.write(section["content"])
        
        # Save code examples to separate files
        code_dir = os.path.join(self.output_dir, "code_examples")
        os.makedirs(code_dir, exist_ok=True)
        
        for example in code_examples:
            ext = example["language"] if example["language"] != "text" else "txt"
            filename = f"example_{example['id']}_{example['purpose']}.{ext}"
            with open(os.path.join(code_dir, filename), "w", encoding="utf-8") as f:
                f.write(example["content"])
    
    def _sanitize_filename(self, filename: str) -> str:
        """
        Sanitize a filename to ensure it's valid across operating systems.
        
        Args:
            filename: The filename to sanitize
            
        Returns:
            Sanitized filename
        """
        import re
        
        # Replace invalid characters with underscores
        sanitized = re.sub(r'[\\/*?:"<>|]', '_', filename)
        # Replace multiple underscores with a single one
        sanitized = re.sub(r'_+', '_', sanitized)
        # Remove leading/trailing underscores
        sanitized = sanitized.strip('_')
        # Ensure the filename is not empty
        if not sanitized:
            sanitized = "unnamed"
        
        return sanitized

async def main():
    """
    Main entry point for the documentation scraper.
    """
    scraper = DocumentationScraper()
    result = await scraper.scrape_documentation()
    
    if "error" in result:
        print(f"Error: {result['error']}")
        return
    
    print(f"Documentation scraped successfully from {result['url']}")
    print(f"Title: {result['title']}")
    print(f"Found {result['structure']['section_count']} sections")
    print(f"Found {len(result['api_sections'])} API sections")
    print(f"Found {len(result['code_examples'])} code examples")
    print(f"Output saved to {result['output_dir']}")

# Run the scraper
if __name__ == "__main__":
    asyncio.run(main())
