# Cline Rules for crawl4ai-rag Project

## Project Patterns

### Code Style
- Use Python 3.9+ features and type hints throughout the codebase
- Follow PEP 8 guidelines for code formatting
- Use docstrings for all public functions, classes, and methods
- Prefer explicit over implicit code
- Use meaningful variable and function names

### Project Structure
- Organize code in a modular fashion with clear separation of concerns
- Keep modules focused on a single responsibility
- Use relative imports within the package
- Place tests in a separate `tests` directory mirroring the package structure

### Development Workflow
- Document changes in Memory Bank files, particularly activeContext.md and progress.md
- Update .clinerules when discovering new patterns or preferences
- Follow test-driven development where appropriate
- Keep commits focused and with descriptive messages
- Always activate the virtual environment before running Python commands:
  - On Windows: `.\.venv\Scripts\activate`
  - On Unix/Mac: `source .venv/bin/activate`

## Technical Decisions

### Dependency Management
- Use uv for package management (preferred over pip or poetry)
- Install packages with `uv pip install <package>`
- Install project in development mode with `uv pip install -e .`
- For packages with binary components (like Playwright), follow up with additional installation steps:
  - Example: `python -m playwright install` after installing playwright
- When adding new dependencies, update pyproject.toml
- Avoid using relative imports across package boundaries to prevent circular imports
- Use absolute imports for cross-package references

### Supabase Integration
- Use the Supabase Python client for database interactions
- Follow the existing schema for the crawl4ai_site_pages table
- Use connection pooling for efficient database access
- Handle connection errors gracefully with retries

### Code Generation
- Use Jinja2 templates for code generation
- Organize templates by website type and scraping purpose
- Include comprehensive error handling in generated code
- Add detailed comments explaining the generated code

### Error Handling
- Use structured logging with loguru
- Implement graceful degradation for partial failures
- Provide detailed error messages with context
- Use custom exception classes for different error types

## User Preferences

### Output Format
- Generated code should be well-formatted and follow PEP 8
- Markdown output should use consistent formatting
- CLI output should be clear and concise
- Error messages should be informative and actionable

### Documentation
- Keep Memory Bank files up-to-date with current status
- Document API changes in activeContext.md
- Track progress in progress.md
- Update technical decisions in .clinerules

## Implementation Notes

### Website Analysis
- Start with basic structure analysis before detailed content extraction
- Consider rate limiting to avoid overloading target websites
- Use existing crawl4ai capabilities where possible
- Implement fallbacks for JavaScript-heavy websites

### Markdown Export
- Preserve original content structure where possible
- Use consistent heading levels
- Include metadata about the source
- Handle different content types appropriately (text, tables, code blocks)

### Testing
- Use pytest for all testing
- Mock external dependencies (Supabase, HTTP requests)
- Use VCR or similar for HTTP request recording/playback
- Include both unit and integration tests

## Known Challenges

### Performance
- Crawling large websites can be time-consuming
- Vector embeddings generation requires API calls
- Supabase has query limits that need to be managed
- Memory usage can be high when processing large websites

### System Compatibility
- Windows console encoding conflicts with Unicode output
  - Default cp1252 encoding can't handle crawl4ai's arrows/characters
  - Colorama ANSI codes conflict with pipe redirection
  - Solutions:
    - Set `PYTHONUTF8=1` and `chcp 65001`
    - Disable Colorama with `NO_COLOR=1`
    - Use `python -X utf8` flag

### Security
- API keys need to be securely managed
  - Store API keys in .env file, never in code
  - Use .env.example as a template with placeholder values
  - Verify .env file exists before running code with `ls -Force .env`
  - Set restricted permissions on .env file
  - Add .env to .gitignore to prevent accidental commits
  - Periodically audit with `git log -- .env` to ensure it hasn't been committed
- User-provided URLs must be validated
- Rate limiting and respectful crawling practices must be followed
- Crawled content may contain sensitive information
- Environment variable handling:
  - Always use pydantic_settings with env_file=".env" configuration
  - Provide clear error messages when required environment variables are missing
  - Use .env.example as documentation for required variables
  - Implement validation checks for environment variables at startup

## Diagram Standards

### Mermaid Syntax Rules
1. **Node Labels**:
   - Use simple identifiers: `[NodeName]` not `[Node Name (Type)]`
   - Example: `CR[CrawlResult]` instead of `CR[CrawlResult Object]`

2. **Flowchart Structure**:
   ```mermaid
   flowchart TD
       A[Parent] --> B[Child]
       B --> C[Grandchild]
   ```

3. **Special Characters**:
   - Avoid: `()`, `-`, `{}` in node names
   - Use underscores for spaces: `page_title` not `page title`

4. **Color Guidelines**:
   - Use high-contrast combinations:
   ```mermaid
   flowchart TD
       A[#black]-.->B[#white]
       A-->C[#yellow]
   ```

## Project Evolution
- Initially focused on basic functionality
- Will expand to more specialized website types
- May add more output formats beyond code and markdown
- Could integrate with other tools and workflows
